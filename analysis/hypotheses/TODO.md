- Hypothesis testing component: the group must formulate a minimum of three distinct research hypotheses and test them using a statistical testing method.

- Validation or regularization: the results of your machine learning-based analysis should be validated using cross-validation or k-fold validation. Projects that apply regularization methods will receive extra recognition.

- For both components, you are required to provide a description of the result and adequate complementary information describing and motivating your process:
Why did you use this statistical test or ML algorithm? Which other tests did you consider or evaluate? What metric(s) did you use to measure success or failure, and why did you use it? What challenges did you face evaluating the model? Did you have to clean or restructure your data?

- What is your interpretation of the results? Do you accept or deny the hypothesis, or are you satisfied with your prediction accuracy? For prediction projects, we expect you to argue why you got the accuracy/success metric you have. Intuitively, how do you react to the results? Are you confident in the results?

- Did you find the results corresponded with your initial belief in the data? If yes/no, why do you think this was the case?

- Do you believe the tools for analysis that you chose were appropriate? If yes/no, why or what method could have been used?

- Was the data adequate for your analysis? If not, what aspects of the data were problematic and how could you have remedied that?





`dir.py`:
The result suggests that there is a significant relationship between whether a movie's director is on the top directors list and the movie's average rating category (High/Low).

There is evidence to suggest that the director being in the top list is not independent of the movie's average rating.

This could mean that movies directed by the most frequent directors in your dataset tend to have different average ratings compared to movies directed by others.